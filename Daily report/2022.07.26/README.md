### (2022년 7월 26일) 결과 분석
7월 25일 학습 시켰던 차량을 10,000번의 에피소드 거친 이후 결과를 확인했다. 

<img src="https://github.com/sh02092/unity-ml-agents-DQN-based/blob/dab711a1f125e8a757988684932ea3831affb4e4/Image/max%20Q-value%20220725.PNG" width="50%" height="50%">

약 80~100 번의 타겟 업데이트 이후 점점 max Q-value의 값이 작아지는 것을 볼 수 있다. 
위 결과에 대한 결과 분석

* Epsilon greedy: 약 90번의 타겟 업데이트를 할 때까지 입실론 그리디의 입실론 값을 최솟값에 수렴하게 하여 더이상 랜덤한 행동을 하지 않도록 바꾸면 어떨까라는 생각이 들어서 계산해본 결과 90 x 1,000(타겟 업데이트 주기) = 900,000 스탭으로 충분히 입실론 값이 최솟값인 0.05에 도달하는데는 문제없는 스텝이다. 

* Overfitting: 너무 모델이 반복적인 초반 도로에만 최적화되게끔 학습이 된 것인지 확인하기 위해 약 90번의 타겟 업데이트 이후 타겟 네트워크를 고정시켜 다시 학습해봐야겠다.

* 차량 에이전트가 한 가지 행동을 5 스텝동안 지속: 2차선에서 달리면서 차량이 학습을 하는 과정에서 핸들을 오른쪽으로 꺾고 5스텝을 진행하면 5스텝이 다하기 전에 리셋이 되지는 않지만 다시 복구할 수 없을 정도로 벽과 가까워지게 되어 곧바로 리셋된다.

<img src="https://github.com/sh02092/unity-ml-agents-DQN-based/blob/dab711a1f125e8a757988684932ea3831affb4e4/Image/driving%20time%20220725.PNG" width="50%" height="50%">

100 에피소드마다 평균치를 내어 차량 에이전트가 주행한 시간을 측정했는데, 꾸준히 증가하며 학습이 완료된 시점에 수렴하는 것이 아니라 가늠하기 힘들 정도로 심하게 편차가 큰 것을 볼 수 있다.

loss 값을 따로 뽑아 max Q-value 값과 비교, 차량 에이전트가 한 가지 행동을 3 스텝동안 지속하도록 변경하여 기존의 10,000번의 에피소드가 아닌 7,000번의 에피소드로 줄여 학습을 진행한다.